name: train
mode: train

resume: false
finetune: false

# batch size for each GPU
batch_size: 128
epochs: 1000

freeze: []

optimizer:
  lr: 1e-3
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 1e-2
  
  monitor_val: val_MAE
  patience: 100
  factor: 0.1
  threshold: 0.01

  clip_value: 0
    
model:
  input_dim: 300
  output_dim: 1
  n_d: 64  # Dimension of the prediction layer
  n_a: 64  # Dimension of the attention layer
  n_steps: 10
  gamma: 1.3
  cat_idxs: []
  cat_dims: []
  cat_emb_dim: 1
  n_independent: 2
  n_shared: 2
  epsilon: 1e-15
  virtual_batch_size: 128
  momentum: 0.02
  mask_type: "sparsemax"